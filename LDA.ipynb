{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP8xXXgPFEm7ZIODvTkHUYC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/waterta0115/LDA_graduattion_thesis/blob/main/LDA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 準備"
      ],
      "metadata": {
        "id": "f6J-BttWCKNc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hIdZVm-SanEU"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "import japanize_matplotlib\n",
        "import arviz as az\n",
        "import matplotlib.dates as mdates\n",
        "import igraph as ig\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import logging\n",
        "from tqdm import tqdm\n",
        "from gensim.models.coherencemodel import CoherenceModel\n",
        "az.style.use(\"arviz-doc\")\n",
        "\n",
        "\n",
        "# フィルタクラスを定義\n",
        "class TopicLogFilter(logging.Filter):\n",
        "    def filter(self, record):\n",
        "        # メッセージに \"topic #\" が含まれていたら False（表示しない）を返す\n",
        "        return \"topic #\" not in record.getMessage()\n",
        "class ExcludeAlphaEtaFilter(logging.Filter):\n",
        "    def filter(self, record):\n",
        "        msg = record.getMessage()\n",
        "        if \"alpha\" in msg or \"eta\" in msg:\n",
        "            return False  # このログは表示しない\n",
        "        return True       # それ以外は表示する\n",
        "\n",
        "# ルートロガーにフィルター追加\n",
        "logger = logging.getLogger()\n",
        "logger.addFilter(ExcludeAlphaEtaFilter())\n",
        "\n",
        "logger = logging.getLogger('gensim.models.ldamodel')\n",
        "logger.addFilter(TopicLogFilter())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import, division, unicode_literals  # noqa\n",
        "import logging\n",
        "import sys\n",
        "import lda._lda\n",
        "import lda.utils\n",
        "from scipy.special import digamma, polygamma\n",
        "\n",
        "# ログ出力の設定(1)\n",
        "logger = logging.getLogger('lda')\n",
        "\n",
        "class LDA:\n",
        "    def __init__(self, n_topics, n_iter=2000, update_alpha=False,update_eta=False,alpha=0.1, eta=0.01, random_state=None,\n",
        "                 refresh=10):\n",
        "        self.n_topics = n_topics\n",
        "        self.n_iter = n_iter\n",
        "        self.update_alpha = update_alpha\n",
        "        self.alpha = alpha\n",
        "        self.update_eta = update_eta\n",
        "        self.eta = eta\n",
        "\n",
        "        # if random_state is None, check_random_state(None) does nothing\n",
        "        # other than return the current numpy RandomState\n",
        "        self.random_state = random_state\n",
        "        self.refresh = refresh\n",
        "\n",
        "        alpha_is_valid = alpha > 0 if isinstance(alpha, (int, float)) else np.all(alpha > 0)\n",
        "        if not alpha_is_valid or eta <= 0:\n",
        "             raise ValueError(\"alpha and eta must be greater than zero\")\n",
        "\n",
        "        # random numbers that are reused\n",
        "        rng = lda.utils.check_random_state(random_state)\n",
        "        self._rands = rng.rand(1024**2 // 8)  # 1MiB of random variates\n",
        "\n",
        "        # configure console logging if not already configured\n",
        "        # ログ出力の設定(2)\n",
        "        if len(logger.handlers) == 1 and isinstance(logger.handlers[0], logging.NullHandler):\n",
        "            logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        self._fit(X)\n",
        "        return self\n",
        "\n",
        "    def fit_transform(self, X, y=None):\n",
        "        if isinstance(X, np.ndarray):\n",
        "            # in case user passes a (non-sparse) array of shape (n_features,)\n",
        "            # turn it into an array of shape (1, n_features)\n",
        "            X = np.atleast_2d(X)\n",
        "        self._fit(X)\n",
        "        return self.doc_topic_\n",
        "\n",
        "    def transform(self, X, max_iter=20, tol=1e-16):\n",
        "        if isinstance(X, np.ndarray):\n",
        "            # in case user passes a (non-sparse) array of shape (n_features,)\n",
        "            # turn it into an array of shape (1, n_features)\n",
        "            X = np.atleast_2d(X)\n",
        "        doc_topic = np.empty((X.shape[0], self.n_topics))\n",
        "            # 結果格納用の配列を用意\n",
        "        WS, DS = lda.utils.matrix_to_lists(X)\n",
        "            # 行列→リスト　WS：単語インデックス　DS：その単語がどの文書に属しているか　に変換\n",
        "        \"\"\"\n",
        "        Doc1: 単語1が2回, 単語3が1回\n",
        "        Doc2: 単語2が3回\n",
        "        \"\"\"\n",
        "        # この場合\n",
        "        # WS = [1, 1, 3, 2, 2, 2]\n",
        "        # DS = [0, 0, 0, 1, 1, 1]\n",
        "        # このようにWSとDSは同じ長さとなる\n",
        "\n",
        "        # TODO: this loop is parallelizable\n",
        "        for d in np.unique(DS):\n",
        "            doc_topic[d] = self._transform_single(WS[DS == d], max_iter, tol)\n",
        "        return doc_topic\n",
        "\n",
        "    def _transform_single(self, doc, max_iter, tol):\n",
        "        # 内部的に doc = WS[DS == d] のような変換が行われている(あるdに属している単語集合)\n",
        "        # ここでdocは「文書中の単語たち」を表す 単語インデックスの配列\n",
        "        PZS = np.zeros((len(doc), self.n_topics))\n",
        "        for iteration in range(max_iter + 1):  # +1 is for initialization\n",
        "            PZS_new = self.components_[:, doc].T\n",
        "                # components_はトピック-単語分布行列(phi)\n",
        "                # その中からdoc内に含まれている単語だけを抽出して転置\n",
        "                # PZS_newは各単語が「どのトピックにどれくらい関係しているか」の確率表\n",
        "            PZS_new *= (PZS.sum(axis=0) - PZS + self.alpha)\n",
        "                # PZS.sum(axis=0)：各文書はどれくらいの数のトピックを持っているか\n",
        "                    # ここではブロードキャスト機能が自動的に働くため、PZSの次元は(1,n_topics)となる\n",
        "                # PZS.sum(axis=0)-PZS：shape==(n_words_in_doc,n_topics)\n",
        "                # self.alphaはスカラー又は(n_topics,)であるが、ブロードキャストにより全体の次元は(n_words_in_doc,n_topics)\n",
        "            PZS_new /= PZS_new.sum(axis=1)[:, np.newaxis]  # vector to single column matrix\n",
        "                # np.array([a,b,c])[:,np.newaxis]とすることで1次元配列を2次元配列にすることが可能\n",
        "                    # (3,)→(3,1)；これらは全くの別物\n",
        "                # ここでは確率にしている（正規化）\n",
        "            delta_naive = np.abs(PZS_new - PZS).sum()\n",
        "                # 前回の値との変化量\n",
        "            logger.debug('transform iter {}, delta {}'.format(iteration, delta_naive))\n",
        "            PZS = PZS_new\n",
        "            if delta_naive < tol:\n",
        "                break\n",
        "                # 収束判定\n",
        "        theta_doc = PZS.sum(axis=0) / PZS.sum()\n",
        "            # 文書内のすべての単語にわたるトピック確率を合計し、全体で正規化して文書レベルのトピック分布を作る\n",
        "        assert len(theta_doc) == self.n_topics\n",
        "        assert theta_doc.shape == (self.n_topics,)\n",
        "        return theta_doc\n",
        "\n",
        "    def _fit(self, X):\n",
        "        \"\"\"inference by \"\"\"\n",
        "        random_state = lda.utils.check_random_state(self.random_state)\n",
        "        rands = self._rands.copy()\n",
        "            # randsは事前に生成された乱数列のコピー\n",
        "            # gibbs samplingで使う乱数をここから供給する\n",
        "        self._initialize(X)\n",
        "            # 各単語にランダムにトピックを割り当てる\n",
        "            # 各カウント行列を作成：nzw_,ndz_,nz_\n",
        "                # nzw_：topic-wordの共起数\n",
        "                # ndz_：doc-topicの共起数\n",
        "                # nz_：各トピックに属する単語数の合計\n",
        "        for it in range(self.n_iter):\n",
        "            # FIXME: using numpy.roll with a random shift might be faster\n",
        "            random_state.shuffle(rands)\n",
        "            if it % self.refresh == 0:\n",
        "                ll = self.loglikelihood()\n",
        "                logger.info(\"<{}> log likelihood: {:.0f}\".format(it, ll))\n",
        "                # keep track of loglikelihoods for monitoring convergence\n",
        "                self.loglikelihoods_.append(ll)\n",
        "            self._sample_topics(rands)\n",
        "            if it == 1:\n",
        "                print(\"ndz 初回サンプル:\",self.ndz_[:5])\n",
        "            if it == 100:\n",
        "                print(\"ndz 100回目:\", self.ndz_[:5])\n",
        "            if self.update_alpha:     # ユーザが制御できるようにオプション化\n",
        "                self._update_alpha()\n",
        "                if it % 10 == 0:\n",
        "                    logger.info(f\"iteration{it}: alpha = {self.alpha}\")\n",
        "            if self.update_eta:     # ユーザが制御できるようにオプション化\n",
        "                self._update_eta()\n",
        "                if it % 10 == 0:\n",
        "                    logger.info(f\"iteration{it}: alpha = {self.eta}\")\n",
        "            # ここがcollapsed gibbs samplingの実体。lda-projectの内部で構築されている\n",
        "        ll = self.loglikelihood()\n",
        "            # 最終的な対数尤度の計算・出力\n",
        "        logger.info(\"<{}> log likelihood: {:.0f}\".format(self.n_iter - 1, ll))\n",
        "        # note: numpy /= is integer division\n",
        "        self.components_ = (self.nzw_ + self.eta).astype(float)\n",
        "            # components_とは各トピックごとの単語分布（phi）：(n_topics,n_words)\n",
        "            # etaを加えることでスムージングしている\n",
        "        self.components_ /= np.sum(self.components_, axis=1)[:, np.newaxis]\n",
        "            # 確率にしている（正規化）\n",
        "        self.topic_word_ = self.components_\n",
        "            # scikit-learn互換用のために同じ内容を別名で保存\n",
        "        self.doc_topic_ = (self.ndz_ + self.alpha).astype(float)\n",
        "            # doc_topic_とは各文書ごとのトピック分布(theta)：(n_docs,n_topics)\n",
        "        self.doc_topic_ /= np.sum(self.doc_topic_, axis=1)[:, np.newaxis]\n",
        "\n",
        "        # delete attributes no longer needed after fitting to save memory and reduce clutter\n",
        "        del self.WS\n",
        "        del self.DS\n",
        "        del self.ZS\n",
        "        return self\n",
        "            # components_とdoc_topic_とloglikelihoods_を返す\n",
        "\n",
        "    def _initialize(self, X):\n",
        "        D, W = X.shape\n",
        "        N = int(X.sum())\n",
        "        n_topics = self.n_topics\n",
        "        n_iter = self.n_iter\n",
        "        logger.info(\"n_documents: {}\".format(D))\n",
        "        logger.info(\"vocab_size: {}\".format(W))\n",
        "        logger.info(\"n_words: {}\".format(N))\n",
        "        logger.info(\"n_topics: {}\".format(n_topics))\n",
        "        logger.info(\"n_iter: {}\".format(n_iter))\n",
        "\n",
        "        self.nzw_ = nzw_ = np.zeros((n_topics, W), dtype=np.intc, order=\"F\")\n",
        "        self.ndz_ = ndz_ = np.zeros((D, n_topics), dtype=np.intc, order=\"C\")\n",
        "        self.nz_ = nz_ = np.zeros(n_topics, dtype=np.intc)\n",
        "\n",
        "        self.WS, self.DS = WS, DS = lda.utils.matrix_to_lists(X)\n",
        "        self.ZS = ZS = np.empty_like(self.WS, dtype=np.intc)\n",
        "        np.testing.assert_equal(N, len(WS))\n",
        "        for i in range(N):\n",
        "            w,d = WS[i], DS[i]\n",
        "            z_new = i % n_topics\n",
        "            ZS[i] = z_new\n",
        "            ndz_[d, z_new] += 1\n",
        "            nzw_[z_new, w] += 1\n",
        "            nz_[z_new] += 1\n",
        "        self.loglikelihoods_ = []\n",
        "\n",
        "    def loglikelihood(self):# ここではalphaやetaはスカラーであることが想定されている\n",
        "        nzw, ndz, nz = self.nzw_, self.ndz_, self.nz_\n",
        "        alpha = self.alpha\n",
        "        eta = self.eta\n",
        "        nd = np.sum(ndz, axis=1).astype(np.intc)\n",
        "        if isinstance(alpha, np.ndarray) and alpha.ndim > 0:\n",
        "            alpha_scalar = alpha[0]\n",
        "        else:\n",
        "            alpha_scalar = alpha\n",
        "        if isinstance(eta, np.ndarray) and eta.ndim > 0:\n",
        "            eta_scalar = eta[0]\n",
        "        else:\n",
        "            eta_scalar = eta\n",
        "        return lda._lda._loglikelihood(nzw, ndz, nz, nd, alpha_scalar, eta_scalar)\n",
        "\n",
        "    def _sample_topics(self, rands):# ここではalphaやetaはK次元であることが想定されている\n",
        "        n_topics, vocab_size = self.nzw_.shape\n",
        "        if isinstance(self.alpha, (int, float)):\n",
        "            alpha_array = np.full(n_topics, self.alpha)\n",
        "        else:\n",
        "            alpha_array = self.alpha\n",
        "        if isinstance(self.eta, (int, float)):\n",
        "            eta_array = np.full(n_topics, self.eta)\n",
        "        else:\n",
        "            eta_array = self.eta\n",
        "        alpha = alpha_array.astype(np.float64)\n",
        "        eta = eta_array.astype(np.float64)\n",
        "        lda._lda._sample_topics(self.WS, self.DS, self.ZS, self.nzw_, self.ndz_, self.nz_,\n",
        "                                alpha, eta, rands)\n",
        "\n",
        "    import numpy as np\n",
        "    from scipy.special import digamma, polygamma\n",
        "    def _update_alpha(self):\n",
        "        if not hasattr(self, \"ndz_\"):\n",
        "            raise ValueError(\"ndz_ が存在しません\")\n",
        "\n",
        "        if np.all(self.ndz_ == self.ndz_[0]):\n",
        "            print(\"警告: ndz_ が全ドキュメント同じ値になっています\")\n",
        "\n",
        "        dt = self.ndz_              # shape (D, K)\n",
        "        D, K = dt.shape\n",
        "\n",
        "        # ensure alpha is scalar\n",
        "        if isinstance(self.alpha, np.ndarray):\n",
        "            alpha_scalar = float(self.alpha[0])\n",
        "        else:\n",
        "            alpha_scalar = float(self.alpha) # float型の場合はそのままfloatに変換\n",
        "\n",
        "        # s = digamma(dt + alpha_scalar).sum(axis=0)\n",
        "        # g = s - D * digamma(alpha_scalar)\n",
        "        # h = -D * polygamma(1, alpha_scalar)\n",
        "        # alpha_new = alpha_scalar - g / h\n",
        "        for i in range(100):\n",
        "            N_d = np.sum(dt,axis=1)\n",
        "            term1_num = np.sum(digamma(dt+alpha_scalar))\n",
        "            term2_num = D*K*digamma(alpha_scalar)\n",
        "            numerator = term1_num - term2_num\n",
        "            term1_den = K*np.sum(digamma(N_d+alpha_scalar*K))\n",
        "            term2_den = D*K*digamma(alpha_scalar*K)\n",
        "            denominator = term1_den - term2_den\n",
        "            alpha_new = alpha_scalar*(numerator/denominator)\n",
        "            diff = np.abs(alpha_new-alpha_scalar)\n",
        "            print(f\"Iter{i+1}:alpha={alpha_new:.6f}(diff={diff:.6e})\")\n",
        "            if diff<1e-05:\n",
        "                print(\"収束しました\")\n",
        "                break\n",
        "            alpha_scalar = alpha_new\n",
        "        # store symmetric vector\n",
        "        self.alpha = np.ones(K)*alpha_new\n",
        "\n",
        "        # logging\n",
        "        logger.info(f\"alpha updated (symmetric): {alpha_new}\")\n",
        "\n",
        "        if not hasattr(self, \"alpha_history\"):\n",
        "            self.alpha_history = []\n",
        "        self.alpha_history.append(self.alpha.copy())\n",
        "    def _update_eta(self):\n",
        "        if not hasattr(self, \"nzw_\"):\n",
        "            raise ValueError(\"nzw_ が存在しません\")\n",
        "\n",
        "        nzw = self.nzw_              # shape (K, V)\n",
        "        nz = self.nz_               # shape (K,)\n",
        "        K, V = nzw.shape\n",
        "\n",
        "        # ensure eta is scalar\n",
        "        if isinstance(self.eta, np.ndarray):\n",
        "            eta_scalar = float(self.eta[0])\n",
        "        else:\n",
        "            eta_scalar = float(self.eta)\n",
        "\n",
        "        # fixed-point iteration (symmetric eta)\n",
        "        for i in range(100):\n",
        "            term1_num = np.sum(digamma(nzw + eta_scalar))     # Σ_k Σ_w ψ(n_kw + η)\n",
        "            term2_num = K * V * digamma(eta_scalar)           # K*V ψ(η)\n",
        "            numerator = term1_num - term2_num\n",
        "\n",
        "            term1_den = V * np.sum(digamma(nz + V * eta_scalar))  # V Σ_k ψ(n_k· + Vη)\n",
        "            term2_den = K * V * digamma(V * eta_scalar)           # K*V ψ(Vη)\n",
        "            denominator = term1_den - term2_den\n",
        "\n",
        "            eta_new = eta_scalar * (numerator / denominator)\n",
        "            diff = abs(eta_new - eta_scalar)\n",
        "\n",
        "            print(f\"[eta] Iter{i+1}: eta={eta_new:.6f} (diff={diff:.6e})\")\n",
        "\n",
        "            if diff < 1e-5:\n",
        "                print(\"eta 収束しました\")\n",
        "                break\n",
        "\n",
        "            eta_scalar = eta_new\n",
        "\n",
        "        # store symmetric eta vector\n",
        "        self.eta = eta_new\n",
        "\n",
        "        # logging\n",
        "        logger.info(f\"eta updated (symmetric): {eta_new}\")\n",
        "\n",
        "        # store history\n",
        "        if not hasattr(self, \"eta_history\"):\n",
        "            self.eta_history = []\n",
        "        self.eta_history.append(eta_new)"
      ],
      "metadata": {
        "id": "Nm4IUrKJauzR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare(df):\n",
        "    from gensim import corpora\n",
        "    texts = df.groupby(\"文書\")[\"単語\"].apply(list).tolist()\n",
        "    filtered_texts = [doc for doc in texts if len(doc) > 2]\n",
        "    dictionary = corpora.Dictionary(filtered_texts)\n",
        "    corpus = [dictionary.doc2bow(text) for text in filtered_texts]\n",
        "    return filtered_texts,dictionary,corpus"
      ],
      "metadata": {
        "id": "5vPEpIa5afSl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_bow(data):\n",
        "    from sklearn.feature_extraction.text import CountVectorizer\n",
        "    import pandas as pd\n",
        "\n",
        "    doc_word_lists = data.groupby(\"文書\")[\"単語\"].apply(list)\n",
        "\n",
        "    filtered_doc_lists = doc_word_lists[doc_word_lists.apply(len) > 2]\n",
        "\n",
        "    docs = filtered_doc_lists.apply(lambda items: ' '.join(map(str, items)))\n",
        "\n",
        "    vectorizer = CountVectorizer(token_pattern='[^ ]+')\n",
        "    bow_matrix = vectorizer.fit_transform(docs)\n",
        "    vocab = vectorizer.get_feature_names_out()\n",
        "\n",
        "    # n_samples, n_features\n",
        "    print(f\"元の文書数（ユニーク）：{len(doc_word_lists)}\")\n",
        "    print(f\"フィルタリング後の文書数：{len(filtered_doc_lists)}\")\n",
        "    print(\"BOW shape:\", bow_matrix.shape)\n",
        "\n",
        "    return bow_matrix, vocab"
      ],
      "metadata": {
        "id": "80AEu4ZvbIq-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perplexity算出のためにトレインテストスプリットを施したもの\n",
        "def make_bow_split(data):\n",
        "    from sklearn.feature_extraction.text import CountVectorizer\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    import pandas as pd\n",
        "\n",
        "    doc_word_lists = data.groupby(\"文書\")[\"単語\"].apply(list)\n",
        "\n",
        "    filtered_doc_lists = doc_word_lists[doc_word_lists.apply(len) > 2]\n",
        "\n",
        "    docs = filtered_doc_lists.apply(lambda items: ' '.join(map(str, items)))\n",
        "\n",
        "    vectorizer = CountVectorizer(token_pattern='[^ ]+')\n",
        "    bow_matrix = vectorizer.fit_transform(docs)\n",
        "    vocab = vectorizer.get_feature_names_out()\n",
        "    X_train, X_test = train_test_split(bow_matrix, test_size=0.2, random_state=42)\n",
        "\n",
        "    # n_samples, n_features\n",
        "    print(f\"元の文書数（ユニーク）：{len(doc_word_lists)}\")\n",
        "    print(f\"フィルタリング後の文書数：{len(filtered_doc_lists)}\")\n",
        "    print(\"BOW shape:\", bow_matrix.shape)\n",
        "\n",
        "    return X_train,X_test,vocab"
      ],
      "metadata": {
        "id": "SVFoCPYPX7PR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "# Xにテストデータを入れる\n",
        "def calculate_perplexity_fast(model, X):\n",
        "    # 1. テストデータのトピック分布を推論 (n_docs, n_topics)\n",
        "    doc_topic_dist = model.transform(X)\n",
        "\n",
        "    # 2. トピック-単語分布を正規化 (n_topics, n_words)\n",
        "    topic_word_dist = model.components_ / model.components_.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "    doc_word_prob = np.dot(doc_topic_dist, topic_word_dist)\n",
        "\n",
        "    epsilon = 1e-10\n",
        "\n",
        "    if hasattr(X, \"toarray\"):\n",
        "        X_arr = X.toarray()\n",
        "    else:\n",
        "        X_arr = X\n",
        "\n",
        "    # 要素ごとの計算\n",
        "    log_likelihood = np.sum(X_arr * np.log(doc_word_prob + epsilon))\n",
        "\n",
        "    total_words = X.sum()\n",
        "    perplexity = np.exp(-log_likelihood / total_words)\n",
        "\n",
        "    return perplexity"
      ],
      "metadata": {
        "id": "6XJsD7eOasCd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 実行"
      ],
      "metadata": {
        "id": "MgledflSCQGG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train,test,vocab = make_bow(data)\n",
        "texts,dictionary,corpus = prepare(data)"
      ],
      "metadata": {
        "id": "a9A_16-RxGyt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ハイパラ更新なしでperplexity測る用にLDAを実行\n",
        "start = 2\n",
        "limit = 30\n",
        "step = 1\n",
        "perplexity_vals = []\n",
        "from tqdm import tqdm\n",
        "for n_topic in tqdm(range(start, limit, step), desc=\"Calculating perplexity\"):\n",
        "    lda = LDA(n_topic,alpha=0.1,eta=0.01,update_alpha=False,update_eta=False, random_state = 0, n_iter = 1500)\n",
        "    lda.fit(train)\n",
        "    perplexity=calculate_perplexity_fast(lda, test)\n",
        "    perplexity_vals.append(perplexity)"
      ],
      "metadata": {
        "id": "aEvHMxvLbfYp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "topic_range = range(2, 16, 1)\n",
        "plt.plot(topic_range, perplexity_vals, marker='o')\n",
        "\n",
        "plt.title(\"テストデータに対するPerplexity\",fontsize=25)\n",
        "plt.xlabel(\"トピック数 (K)\",fontsize=20)\n",
        "plt.ylabel(\"Perplexity\",fontsize=20)\n",
        "plt.grid(True)\n",
        "plt.tick_params(axis='x', labelsize=20)  # 横軸\n",
        "plt.tick_params(axis='y', labelsize=20)\n",
        "plt.savefig(\"/home/jovyan/fig/perp_loyal.pdf\",dpi=300, bbox_inches='tight')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mryX4Qs2b29f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 例えばトピックの候補８においてLDAを実行する\n",
        "lda8 = LDA(n_topics = 8,alpha=0.1,eta=0.01,update_alpha=True,update_eta=True, random_state = 0, n_iter = 1500)\n",
        "lda8.fit(train)\n",
        "# 他にも候補点においてLDAを実行する。"
      ],
      "metadata": {
        "id": "fcl94-lzerjR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        },
        "outputId": "05e2ff20-0cba-4ba3-dd67-c83c27ec89a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'LDA' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1420663475.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mloyal_uni8\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLDA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_topics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0meta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mupdate_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mupdate_eta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mloyal_uni8\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# 他にも候補点においてLDAを実行する。\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'LDA' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ハイパラが収束しているのかの確認\n",
        "start = 1000\n",
        "end = 1500\n",
        "# 平均値を計算\n",
        "eta_mean = np.mean(lda8.eta_history[start:end])\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "# alpha history\n",
        "ax.plot(lda8.eta_history, label=\"eta history\")\n",
        "\n",
        "# 平均線（1000〜1999）\n",
        "ax.axhline(eta_mean, linestyle='--', alpha=0.7,\n",
        "           label=f\"mean (1000-1500) = {eta_mean:.4f}\")\n",
        "\n",
        "# タイトル・軸\n",
        "ax.set_title(\"Eta History of loyal\", fontsize=24)\n",
        "ax.tick_params(axis='x', labelsize=20)\n",
        "ax.tick_params(axis='y', labelsize=20)\n",
        "ax.grid(True, linestyle='--', alpha=0.5)\n",
        "\n",
        "ax.legend(fontsize=17)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"/home/jovyan/fig/loyal8_eta_history.png\",dpi=300, bbox_inches='tight')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "36qh8cQ8e2Qt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Coherenceを計測\n",
        "topic_word_lda8 = lda8.topic_word_   # shape (n_topics, vocab)\n",
        "topics_lda8 = []\n",
        "n_top_words = 5\n",
        "n_topic = 8\n",
        "coherence_vals=[]\n",
        "for t in range(n_topic):\n",
        "    # topn_uni = topic_word_uni[t].argsort()[:-n_top_words - 1:-1]\n",
        "    # topics_uni.append([vocab_uni[w_id] for w_id in topn_uni])\n",
        "    word_ids = topic_word_lda8[t].argsort()[-n_top_words:][::-1]\n",
        "    word_ids = [int(i) for i in word_ids]  # numpy型→python int\n",
        "    words = [vocab[i] for i in word_ids]\n",
        "    topics_lbr8.append(words)\n",
        "cm = CoherenceModel(topics=topics_lda8, texts=texts, dictionary=dictionary, coherence='c_npmi')\n",
        "coherence_vals.append(cm.get_coherence())"
      ],
      "metadata": {
        "id": "tcB8VVT6fHSN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 候補8,10,12,14でのcoherenceの可視化（比較）\n",
        "x = [8,10,12,14]\n",
        "c1 = 'darkturquoise'\n",
        "plt.plot(x, coherence_vals, 'o-', color=c1)\n",
        "plt.xlabel('トピック数(K)',fontsize=25)\n",
        "plt.ylabel('Coherence(NPMI)',fontsize=25)\n",
        "plt.tick_params(axis='x', labelsize=20)  # 横軸\n",
        "plt.tick_params(axis='y', labelsize=20)  # 縦軸\n",
        "plt.legend(fontsize=20)\n",
        "plt.grid(True, linestyle='--', alpha=0.5)\n",
        "plt.savefig(\"/home/jovyan/fig/coherence_loyal.pdf\",dpi=300, bbox_inches='tight')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XbsFIeI4fm-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Coherenceで最も値が大きいのが最適トピック数となる。最適トピック数は10であったと仮定して、10でLDAを実行する。"
      ],
      "metadata": {
        "id": "L8ZLF61Cfy63"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lda10 = LDA(n_topics = 8,alpha=0.1,eta=0.01,update_alpha=False,update_eta=False, random_state = 0, n_iter = 1500)\n",
        "lda10.fit(train)\n",
        "# alphaとetaの値は学習・更新した際の値を使う。"
      ],
      "metadata": {
        "id": "hSDFYbj9vfbN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pyLDAvis\n",
        "\n",
        "tt_10 = lda10.topic_word_   # shape: (n_topics, n_terms) term distribution\n",
        "dt_10 = lda10.doc_topic_     # shape: (n_docs, n_topics) topic distribution\n",
        "term_frequency = np.asarray(train.sum(axis=0)).flatten()  # 各単語の出現回数\n",
        "\n",
        "# --- pyLDAvis形式に変換して可視化 ---\n",
        "vis_data_10 = pyLDAvis.prepare(\n",
        "    topic_term_dists=tt_10,\n",
        "    doc_topic_dists=dt_10,\n",
        "    doc_lengths=train.sum(axis=1).A1,\n",
        "    vocab=vocab,\n",
        "    term_frequency=term_frequency\n",
        ")\n",
        "pyLDAvis.display(vis_data_10)"
      ],
      "metadata": {
        "id": "KW1LZytrgKDT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 単語分布の保存\n",
        "column_names = ['topic1',\"topic2\",\"topic3\",\"topic4\",\"topic5\",\"topic6\",\n",
        "               \"topic7\",\"topic8\",\"topic9\",\"topic10\"]\n",
        "\n",
        "tt_df = pd.DataFrame(\n",
        "    data=tt_10,\n",
        "    index=column_names,\n",
        "    columns=vocab\n",
        ")\n",
        "tt_df.to_csv(\"h2o/matrix/tt_loyal.csv\", encoding='utf-8', index=True)"
      ],
      "metadata": {
        "id": "7k5B_jpdg9hA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# トピック分布の保存\n",
        "matrix_length = dt_10.shape[0]\n",
        "new_index_list = list(range(matrix_length))\n",
        "\n",
        "dt_df = pd.DataFrame(\n",
        "    data=dt_10,\n",
        "    index=new_index_list,\n",
        "    columns=column_names\n",
        ")\n",
        "dt_df.to_csv(\"h2o/matrix/dt_loyal.csv\", encoding='utf-8', index=False)"
      ],
      "metadata": {
        "id": "8n8lAiHShrwW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ネットワーク分析"
      ],
      "metadata": {
        "id": "3wdht5qXg644"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 準備"
      ],
      "metadata": {
        "id": "0j3_vi4iqt2I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def adj_matrix_dt(topic_term_dists):\n",
        "    matrix = np.dot(topic_term_dists.T, topic_term_dists)\n",
        "    if matrix.max() != matrix.min():\n",
        "        matrix = (matrix - matrix.min()) / (matrix.max() - matrix.min())\n",
        "    return matrix"
      ],
      "metadata": {
        "id": "uifvi7FVg5fh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "def find_threshold_by_count(matrix, target_edge_count):\n",
        "    \"\"\"\n",
        "    エッジの重み行列から、指定した「残したい本数」になるような閾値を計算し、\n",
        "    CDF（累積分布関数）とともに可視化する関数。\n",
        "\n",
        "    Args:\n",
        "        matrix (np.ndarray): 接続行列（隣接行列）\n",
        "        target_edge_count (int): 最終的に残したいエッジの本数\n",
        "\n",
        "    Returns:\n",
        "        tau (float): 計算された閾値\n",
        "    \"\"\"\n",
        "\n",
        "    # --- 1. データ抽出 ---\n",
        "    # 上三角行列（対角成分除く）からインデックスを取得し、重複を避ける\n",
        "    indices = np.triu_indices(len(matrix), k=1)\n",
        "    edge_weights = matrix[indices]\n",
        "\n",
        "    # 0より大きい（実際に存在する）エッジの重みだけを抽出\n",
        "    existing_edges = edge_weights[edge_weights > 0]\n",
        "\n",
        "    if len(existing_edges) == 0:\n",
        "        print(\"エラー: グラフにエッジが存在しません。\")\n",
        "        return 0.0\n",
        "\n",
        "    # --- 2. 閾値の逆算ロジック ---\n",
        "    # エッジの重みを昇順（小さい順）にソート\n",
        "    sorted_weights = np.sort(existing_edges)\n",
        "    total_edges = len(existing_edges)\n",
        "\n",
        "    # 目標本数が全エッジ数を超えている場合のガード処理\n",
        "    if target_edge_count >= total_edges:\n",
        "        print(\"注意: 指定された本数が全エッジ数以上です。すべてのエッジを残します。\")\n",
        "        tau = 0.0 # すべて残すために0（または最小値より小さい値）にする\n",
        "        # グラフ表示用のカット率\n",
        "        calculated_cut_percentage = 0.0\n",
        "    elif target_edge_count <= 0:\n",
        "        print(\"注意: 指定された本数が0以下です。すべてのエッジを削除します。\")\n",
        "        tau = sorted_weights[-1] + 1.0 # 最大値より大きくする\n",
        "        calculated_cut_percentage = 100.0\n",
        "    else:\n",
        "        # [小さい ...... 大きい] と並んでいる\n",
        "        # 上位 N個 を残すには、後ろから N番目 の要素の手前を閾値にする\n",
        "        # 例: 全100個, 残したい10個 -> index 0~89 (90個) をカット -> index 89 の値が閾値\n",
        "        cutoff_index = total_edges - target_edge_count - 1\n",
        "\n",
        "        # 閾値を決定（この値より大きいものを残す、とするのが一般的）\n",
        "        tau = sorted_weights[cutoff_index]\n",
        "\n",
        "        # カットされる割合を逆算（グラフ表示用）\n",
        "        calculated_cut_percentage = (cutoff_index + 1) / total_edges * 100\n",
        "\n",
        "    # --- 3. 結果の確認 ---\n",
        "    # 実際にこの閾値でカットした場合に残る数を計算（同じ値が複数ある場合、ズレることがあるため確認）\n",
        "    actual_kept_edges = np.sum(existing_edges > tau)\n",
        "\n",
        "    print(f\"--- 閾値計算結果 (Target Count Mode) ---\")\n",
        "    print(f\"全エッジ数（0より大） : {total_edges}\")\n",
        "    print(f\"残したい目標本数      : {target_edge_count}\")\n",
        "    print(f\"決定された閾値 (τ)    : {tau:.10f}\")\n",
        "    print(f\"---\")\n",
        "    print(f\"実際に残るエッジ数    : {actual_kept_edges} (閾値 {tau:.10f} より大きいもの)\")\n",
        "    print(f\"実際に残る割合        : {actual_kept_edges / total_edges * 100:.2f}%\")\n",
        "    print(f\"計算上のカット率      : {calculated_cut_percentage:.2f}%\")\n",
        "\n",
        "    if actual_kept_edges != target_edge_count:\n",
        "        print(f\"※注意: 重みが同じエッジが存在するため、目標本数({target_edge_count})と完全に一致しませんでした。\")\n",
        "\n",
        "    # --- 4. 累積確率プロット（CDF）の作成と可視化 ---\n",
        "\n",
        "    # Y軸（累積確率）を作成 (1/N, 2/N, ..., N/N)\n",
        "    y_cumulative = np.arange(1, len(sorted_weights) + 1) / len(sorted_weights)\n",
        "\n",
        "    plt.figure(figsize=(12, 7))\n",
        "    # 累積確率プロット (CDF)\n",
        "    plt.plot(sorted_weights, y_cumulative, color='blue', label='Cumulative Distribution (CDF)')\n",
        "\n",
        "    # --- 5. 決定した閾値をグラフ上に表示 ---\n",
        "    # 閾値 τ で垂直線を引く\n",
        "    plt.axvline(x=tau, color='red', linestyle='--',\n",
        "                label=f'Threshold τ = {tau:.10f}')\n",
        "\n",
        "    # 累積確率 (カット率) の点で水平線を引く\n",
        "    # y軸は 0.0 ~ 1.0 なので、パーセンテージを100で割る\n",
        "    plt.axhline(y=calculated_cut_percentage / 100.0, color='red', linestyle=':',\n",
        "                label=f'Cutoff Probability {calculated_cut_percentage/100:.2f} (Removes bottom {calculated_cut_percentage:.1f}%)')\n",
        "\n",
        "    # 交点に印をつける\n",
        "    plt.plot(tau, calculated_cut_percentage / 100.0, 'ro')\n",
        "\n",
        "    plt.title(f'CDF of Edge Weights (Targeting top {target_edge_count} edges)',fontsize=20)\n",
        "    plt.xlabel('Edge Weight (要素の値)',fontsize=20)\n",
        "    plt.ylabel('Cumulative Probability (累積確率)',fontsize=20)\n",
        "    plt.legend(fontsize=20)\n",
        "    plt.grid(True, which=\"both\", ls=\"--\", alpha=0.5)\n",
        "\n",
        "    # Y軸をパーセンテージ表示にする\n",
        "    plt.gca().yaxis.set_major_formatter(ticker.PercentFormatter(xmax=1.0))\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    return tau"
      ],
      "metadata": {
        "id": "bbJKFah3qaKH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import igraph as ig\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_topic_correlation_filtered(adj_matrix, topic_labels, file_path, threshold):\n",
        "    \"\"\"\n",
        "    閾値以下のエッジを完全に削除して可視化する\n",
        "    \"\"\"\n",
        "    matrix = adj_matrix.copy()\n",
        "    np.fill_diagonal(matrix, 0)\n",
        "\n",
        "    # 1. 正規化 (0-1)\n",
        "    if matrix.max() != matrix.min():\n",
        "        matrix = (matrix - matrix.min()) / (matrix.max() - matrix.min())\n",
        "\n",
        "    # 2. 閾値以下の値を0にする（エッジを張らない）\n",
        "    matrix[matrix < threshold] = 0\n",
        "\n",
        "    # 3. グラフの作成（重みが0の箇所にはエッジが張られない）\n",
        "    g = ig.Graph.Weighted_Adjacency(matrix.tolist(), mode=\"undirected\")\n",
        "    layout = g.layout(\"fr\")\n",
        "\n",
        "# --- 重なり回避のためのパラメータ設定 ---\n",
        "    visual_style = {}\n",
        "\n",
        "    # ① ノードラベルを外側に配置する設定\n",
        "    visual_style[\"vertex_label\"] = topic_labels\n",
        "    visual_style[\"vertex_label_dist\"] = 1.4      # ラベルをノードの中心から離す (重要)\n",
        "    visual_style[\"vertex_label_degree\"] = np.pi/4 # ラベルを出す方向 (真上なら -np.pi/2)\n",
        "    visual_style[\"vertex_label_size\"] = 20\n",
        "    visual_style[\"vertex_size\"] = 50\n",
        "\n",
        "    # ② エッジラベル（数値）の視認性向上\n",
        "    if g.ecount() > 0:\n",
        "        visual_style[\"edge_label\"] = [f\"{w:.2f}\" for w in g.es[\"weight\"]]\n",
        "        # エッジラベルを線の中央から少しずらす機能はigraphにはないため、\n",
        "        # 色を薄くし、フォントサイズを下げることで「背景」化させます\n",
        "        visual_style[\"edge_label_size\"] = 15\n",
        "        visual_style[\"edge_label_color\"] = \"blue\"\n",
        "        visual_style[\"edge_width\"] = [w * 2 for w in g.es[\"weight\"]]\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(10, 10))\n",
        "    ig.plot(g, layout=layout, target=ax, **visual_style)\n",
        "    plt.savefig(file_path,dpi=300, bbox_inches='tight')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "ku9DXEiIrWa7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 実行"
      ],
      "metadata": {
        "id": "47r4Zm1xqyRf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dt_df_loyal = pd.read_csv(\"h2o/matrix/dt_loyal.csv\")\n",
        "dt_loyal= dt_df_loyal.to_numpy()"
      ],
      "metadata": {
        "id": "GCwpW2qnrNdT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "adj_loyal = adj_matrix_dt(dt_loyal)\n",
        "labels_loy = [f\"Topic {i+1}\" for i in range(adj_loyal.shape[0])]"
      ],
      "metadata": {
        "id": "r_Drsy5Bq0tA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import igraph as ig\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_topic_correlation_filtered(adj_matrix, topic_labels, file_path, threshold):\n",
        "    \"\"\"\n",
        "    閾値以下のエッジを完全に削除して可視化する\n",
        "    \"\"\"\n",
        "    matrix = adj_matrix.copy()\n",
        "    np.fill_diagonal(matrix, 0)\n",
        "\n",
        "    # 1. 正規化 (0-1)\n",
        "    if matrix.max() != matrix.min():\n",
        "        matrix = (matrix - matrix.min()) / (matrix.max() - matrix.min())\n",
        "\n",
        "    # 2. 閾値以下の値を0にする（エッジを張らない）\n",
        "    matrix[matrix < threshold] = 0\n",
        "\n",
        "    # 3. グラフの作成（重みが0の箇所にはエッジが張られない）\n",
        "    g = ig.Graph.Weighted_Adjacency(matrix.tolist(), mode=\"undirected\")\n",
        "    layout = g.layout(\"fr\")\n",
        "\n",
        "# --- 重なり回避のためのパラメータ設定 ---\n",
        "    visual_style = {}\n",
        "\n",
        "    # ① ノードラベルを外側に配置する設定\n",
        "    visual_style[\"vertex_label\"] = topic_labels\n",
        "    visual_style[\"vertex_label_dist\"] = 1.4      # ラベルをノードの中心から離す (重要)\n",
        "    visual_style[\"vertex_label_degree\"] = np.pi/4 # ラベルを出す方向 (真上なら -np.pi/2)\n",
        "    visual_style[\"vertex_label_size\"] = 20\n",
        "    visual_style[\"vertex_size\"] = 50\n",
        "\n",
        "    # ② エッジラベル（数値）の視認性向上\n",
        "    if g.ecount() > 0:\n",
        "        visual_style[\"edge_label\"] = [f\"{w:.2f}\" for w in g.es[\"weight\"]]\n",
        "        # エッジラベルを線の中央から少しずらす機能はigraphにはないため、\n",
        "        # 色を薄くし、フォントサイズを下げることで「背景」化させます\n",
        "        visual_style[\"edge_label_size\"] = 15\n",
        "        visual_style[\"edge_label_color\"] = \"blue\"\n",
        "        visual_style[\"edge_width\"] = [w * 2 for w in g.es[\"weight\"]]\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(10, 10))\n",
        "    ig.plot(g, layout=layout, target=ax, **visual_style)\n",
        "    plt.savefig(file_path,dpi=300, bbox_inches='tight')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "8Mm7SPESrCMS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_topic_correlation_filtered(adj_loyal,labels_loy,\n",
        "                                file_path = \"/home/jovyan/fig/network_dt_loyal.pdf\",threshold = 0.23)"
      ],
      "metadata": {
        "id": "YvRV5ML0rhnV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 補足〜ブランドネットワークの構築〜"
      ],
      "metadata": {
        "id": "DIcRFeXsrplr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 準備"
      ],
      "metadata": {
        "id": "8fB6olUBthA4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 隣接行列の作成\n",
        "def adj_matrix_(topic_term_dists,vocab,threshold):\n",
        "    co_occurrence = np.dot(topic_term_dists.T, topic_term_dists)\n",
        "    co_matrix = pd.DataFrame(co_occurrence, index=vocab, columns=vocab)\n",
        "    threshold = np.quantile(co_matrix.values, threshold)\n",
        "    filtered = co_matrix.mask(co_matrix < threshold, 0)\n",
        "    adj_matrix = filtered.to_numpy(dtype=float)\n",
        "    return adj_matrix"
      ],
      "metadata": {
        "id": "i3G-mAeYrxXc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_threshold(matrix,cut_percentage):\n",
        "    # --- 2. 閾値計算のためのデータ抽出 ---\n",
        "    import matplotlib.ticker as ticker\n",
        "\n",
        "    # Φ (行列) から、0より大きいエッジの重みだけをすべて取り出す\n",
        "    # (上三角行列または下三角行列のみを対象にし、重複カウントを避ける)\n",
        "    indices = np.triu_indices(len(matrix), k=1) # k=1で対角成分を除く\n",
        "    edge_weights = matrix[indices]\n",
        "\n",
        "    # 0より大きい（実際に存在する）エッジの重みだけを抽出\n",
        "    existing_edges = edge_weights[edge_weights > 0]\n",
        "\n",
        "    if len(existing_edges) == 0:\n",
        "        print(\"エラー: グラフにエッジが存在しません。\")\n",
        "    else:\n",
        "        # --- 3. 閾値の決定 (アプローチ1) ---\n",
        "        # np.percentile を使い、指定したパーセンタイルの値を計算する\n",
        "        # これが求める閾値 τ (tau) となる\n",
        "        # (95パーセンタイルの値 = 下から数えて95%地点の値)\n",
        "        tau = np.percentile(existing_edges, cut_percentage)\n",
        "\n",
        "        print(f\"--- 閾値計算結果 ---\")\n",
        "        print(f\"全エッジ数（0より大）: {len(existing_edges)}\")\n",
        "        print(f\"弱いエッジを {cut_percentage}% カットする場合...\")\n",
        "        print(f\"決定された閾値 (τ): {tau:.7f}\")\n",
        "        print(f\"この閾値 τ より大きい（残すべき）エッジの数: {np.sum(existing_edges > tau)}\")\n",
        "        print(f\"残るエッジの割合: {np.sum(existing_edges > tau) / len(existing_edges) * 100:.2f}%\")\n",
        "\n",
        "\n",
        "        # --- 4. 累積確率プロット（CDF）の作成と可視化 ---\n",
        "\n",
        "        sorted_weights = np.sort(existing_edges)\n",
        "        # Y軸（累積確率）を作成 (1/N, 2/N, ..., N/N)\n",
        "        y_cumulative = np.arange(1, len(sorted_weights) + 1) / len(sorted_weights)\n",
        "\n",
        "        plt.figure(figsize=(12, 7))\n",
        "        # 累積確率プロット (CDF)\n",
        "        plt.plot(sorted_weights, y_cumulative, color='blue', label='Cumulative Distribution (CDF)')\n",
        "\n",
        "        # X軸を対数スケールにすると、べき乗則の「裾」が見やすくなる (オプション)\n",
        "        # plt.xscale('log')\n",
        "\n",
        "        # --- 5. 決定した閾値をグラフ上に表示 ---\n",
        "        # 閾値 τ で垂直線を引く\n",
        "        plt.axvline(x=tau, color='red', linestyle='--',\n",
        "                    label=f'Threshold τ = {tau:.7f}')\n",
        "\n",
        "        # 累積確率 cut_percentage/100 の点で水平線を引く\n",
        "        plt.axhline(y=cut_percentage / 100.0, color='red', linestyle=':',\n",
        "                    label=f'Cumulative Probability {cut_percentage / 100.0:.2f}')\n",
        "\n",
        "        # 交点に印をつける\n",
        "        plt.plot(tau, cut_percentage / 100.0, 'ro') # 'ro' = red circle marker\n",
        "\n",
        "        plt.title('Cumulative Distribution Function (CDF) of Edge Weights')\n",
        "        plt.xlabel('Edge Weight (要素の値)')\n",
        "        plt.ylabel('Cumulative Probability (累積確率)')\n",
        "        plt.legend()\n",
        "        plt.grid(True, which=\"both\", ls=\"--\", alpha=0.5)\n",
        "\n",
        "        # Y軸をパーセンテージ表示にすると見やすい\n",
        "        plt.gca().yaxis.set_major_formatter(ticker.PercentFormatter(xmax=1.0))\n",
        "\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "nsyZuoFCr8G1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ノードの次数に応じてノードサイズを変更\n",
        "def plot_igraph_from_adjacency_deg(\n",
        "    adjacency_matrix,\n",
        "    labels,\n",
        "    layout_type=\"fr\",\n",
        "    vertex_size=30,\n",
        "    edge_width_scale=5,\n",
        "    vertex_label_size=14,\n",
        "    edge_curved=False,\n",
        "    figsize=(8, 8),\n",
        "    edge_threshold=0.0,\n",
        "    vertex_color=\"skyblue\",\n",
        "    edge_color=\"gray\",\n",
        "    top_n_labels=20\n",
        "):\n",
        "    import igraph as ig\n",
        "    import matplotlib.pyplot as plt\n",
        "    import numpy as np\n",
        "\n",
        "    # 隣接行列をnumpy配列に変換\n",
        "    adj = np.array(adjacency_matrix)\n",
        "    n = adj.shape[0]\n",
        "\n",
        "    # エッジの重みが閾値以下のものは0にする\n",
        "    adj = np.where(adj > edge_threshold, adj, 0)\n",
        "\n",
        "    # igraphグラフ作成\n",
        "    g = ig.Graph.Weighted_Adjacency(adj.tolist(), mode=ig.ADJ_UNDIRECTED, attr=\"weight\", loops=False)\n",
        "    g.vs[\"name\"] = labels\n",
        "\n",
        "    # 孤立ノード除去\n",
        "    isolated_nodes = [v.index for v in g.vs if g.degree(v) == 0.0]\n",
        "    if isolated_nodes:\n",
        "        g.delete_vertices(isolated_nodes)\n",
        "\n",
        "\n",
        "\n",
        "    # ノードラベル\n",
        "    # labels = vocab として渡される前提\n",
        "    if \"name\" in g.vs.attributes():\n",
        "        # g.vs[\"name\"] から g.vs[\"label\"] へコピー\n",
        "        g.vs[\"label\"] = g.vs[\"name\"]\n",
        "    else:\n",
        "        # \"name\" がない場合のフォールバック\n",
        "        g.vs[\"label\"] = [str(i) for i in range(g.vcount())]\n",
        "    # 次数の高いノードの上位top_n_labels件のみラベルを付与\n",
        "    degrees = g.degree()\n",
        "    top_indices = np.argsort(degrees)[-top_n_labels:]  # 上位top_n_labels件\n",
        "    g.vs[\"label\"] = [\n",
        "        g.vs[i][\"label\"] if i in top_indices else \"\" for i in range(g.vcount())\n",
        "    ]\n",
        "    # レイアウト\n",
        "    layout = g.layout(layout_type)\n",
        "\n",
        "    # エッジの太さ\n",
        "    edge_weights = g.es[\"weight\"] if \"weight\" in g.es.attributes() else [1]*g.ecount()\n",
        "    if len(edge_weights) > 0 and max(edge_weights) > 0:\n",
        "        edge_widths = [edge_width_scale * (w / max(edge_weights)) for w in edge_weights]\n",
        "    else:\n",
        "        edge_widths = [1 for _ in edge_weights]\n",
        "\n",
        "    # コミュニティ検出（例: Louvain法, fallback to fastgreedy if Louvain not available）\n",
        "    try:\n",
        "        communities = g.community_multilevel(weights=g.es['weight'] if 'weight' in g.es.attributes() else None)\n",
        "    except AttributeError:\n",
        "        # fallback method\n",
        "        communities = g.community_fastgreedy(weights=g.es['weight'] if 'weight' in g.es.attributes() else None).as_clustering()\n",
        "    membership = communities.membership\n",
        "    import matplotlib\n",
        "    cmap = matplotlib.cm.get_cmap('tab20')\n",
        "    num_colors = cmap.N if hasattr(cmap, \"N\") else 20\n",
        "    comm_vertex_colors = [matplotlib.colors.to_hex(cmap(i % num_colors)) for i in membership]\n",
        "    g.vs[\"color\"] = comm_vertex_colors\n",
        "    # ノードの次数に応じてノードサイズを変更する\n",
        "    degrees = g.degree()\n",
        "    # 適度なスケーリングのために最小サイズと最大サイズを指定\n",
        "    min_size = 20\n",
        "    max_size = 80\n",
        "    # スケーリング\n",
        "    if degrees:\n",
        "        min_deg = min(degrees)\n",
        "        max_deg = max(degrees) if max(degrees) != min(degrees) else min(degrees) + 1\n",
        "        # ノードサイズ算出\n",
        "        scaled_sizes = [\n",
        "            min_size + (deg - min_deg) / (max_deg - min_deg) * (max_size - min_size)\n",
        "            for deg in degrees\n",
        "        ]\n",
        "    else:\n",
        "        scaled_sizes = [min_size for _ in g.vs]\n",
        "    g.vs[\"size\"] = scaled_sizes\n",
        "\n",
        "    # ノードサイズに基づき再描画（ノードの大きさ＝次数）\n",
        "    fig, ax = plt.subplots(figsize=figsize)\n",
        "    ig.plot(\n",
        "        g,\n",
        "        target=ax,\n",
        "        layout=layout,\n",
        "        vertex_size=g.vs[\"size\"],\n",
        "        vertex_label=g.vs[\"label\"],\n",
        "        vertex_label_size=vertex_label_size,\n",
        "        vertex_color=g.vs[\"color\"],\n",
        "        edge_width=edge_widths,\n",
        "        edge_color=edge_color,\n",
        "        edge_curved=edge_curved,\n",
        "        bbox=(figsize[0]*100, figsize[1]*100),\n",
        "        margin=40,\n",
        "    )\n",
        "    plt.show()\n",
        "    return g,communities"
      ],
      "metadata": {
        "id": "XiQxWAxisA2B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ノードの媒介中心性指標に応じてノードサイズを変更\n",
        "def plot_igraph_from_adjacency_bet(\n",
        "    adjacency_matrix,\n",
        "    labels,\n",
        "    layout_type=\"fr\",\n",
        "    vertex_size=30,\n",
        "    edge_width_scale=5,\n",
        "    vertex_label_size=14,\n",
        "    edge_curved=False,\n",
        "    figsize=(8, 8),\n",
        "    edge_threshold=0.0,\n",
        "    vertex_color=\"skyblue\",\n",
        "    edge_color=\"gray\",\n",
        "    top_n_labels=20\n",
        "):\n",
        "    import igraph as ig\n",
        "    import matplotlib.pyplot as plt\n",
        "    import numpy as np\n",
        "\n",
        "    # 隣接行列をnumpy配列に変換\n",
        "    adj = np.array(adjacency_matrix)\n",
        "    n = adj.shape[0]\n",
        "\n",
        "    # エッジの重みが閾値以下のものは0にする\n",
        "    adj = np.where(adj > edge_threshold, adj, 0)\n",
        "\n",
        "    # igraphグラフ作成\n",
        "    g = ig.Graph.Weighted_Adjacency(adj.tolist(), mode=ig.ADJ_UNDIRECTED, attr=\"weight\", loops=False)\n",
        "    g.vs[\"name\"] = labels\n",
        "\n",
        "    # 孤立ノード除去\n",
        "    isolated_nodes = [v.index for v in g.vs if g.degree(v) == 0.0]\n",
        "    if isolated_nodes:\n",
        "        g.delete_vertices(isolated_nodes)\n",
        "\n",
        "\n",
        "\n",
        "    # ノードラベル\n",
        "    # labels = vocab として渡される前提\n",
        "    if \"name\" in g.vs.attributes():\n",
        "        # g.vs[\"name\"] から g.vs[\"label\"] へコピー\n",
        "        g.vs[\"label\"] = g.vs[\"name\"]\n",
        "    else:\n",
        "        # \"name\" がない場合のフォールバック\n",
        "        g.vs[\"label\"] = [str(i) for i in range(g.vcount())]\n",
        "    # 次数の高いノードの上位top_n_labels件のみラベルを付与\n",
        "    degrees = g.betweenness()\n",
        "    top_indices = np.argsort(degrees)[-top_n_labels:]  # 上位top_n_labels件\n",
        "    g.vs[\"label\"] = [\n",
        "        g.vs[i][\"label\"] if i in top_indices else \"\" for i in range(g.vcount())\n",
        "    ]\n",
        "    # レイアウト\n",
        "    layout = g.layout(layout_type)\n",
        "\n",
        "    # エッジの太さ\n",
        "    edge_weights = g.es[\"weight\"] if \"weight\" in g.es.attributes() else [1]*g.ecount()\n",
        "    if len(edge_weights) > 0 and max(edge_weights) > 0:\n",
        "        edge_widths = [edge_width_scale * (w / max(edge_weights)) for w in edge_weights]\n",
        "    else:\n",
        "        edge_widths = [1 for _ in edge_weights]\n",
        "\n",
        "    # コミュニティ検出（例: Louvain法, fallback to fastgreedy if Louvain not available）\n",
        "    try:\n",
        "        communities = g.community_multilevel(weights=g.es['weight'] if 'weight' in g.es.attributes() else None)\n",
        "    except AttributeError:\n",
        "        # fallback method\n",
        "        communities = g.community_fastgreedy(weights=g.es['weight'] if 'weight' in g.es.attributes() else None).as_clustering()\n",
        "    membership = communities.membership\n",
        "    import matplotlib\n",
        "    cmap = matplotlib.cm.get_cmap('tab20')\n",
        "    num_colors = cmap.N if hasattr(cmap, \"N\") else 20\n",
        "    comm_vertex_colors = [matplotlib.colors.to_hex(cmap(i % num_colors)) for i in membership]\n",
        "    g.vs[\"color\"] = comm_vertex_colors\n",
        "    # ノードの次数に応じてノードサイズを変更する\n",
        "    degrees = g.betweenness()\n",
        "    # 適度なスケーリングのために最小サイズと最大サイズを指定\n",
        "    min_size = 20\n",
        "    max_size = 80\n",
        "    # スケーリング\n",
        "    if degrees:\n",
        "        min_deg = min(degrees)\n",
        "        max_deg = max(degrees) if max(degrees) != min(degrees) else min(degrees) + 1\n",
        "        # ノードサイズ算出\n",
        "        scaled_sizes = [\n",
        "            min_size + (deg - min_deg) / (max_deg - min_deg) * (max_size - min_size)\n",
        "            for deg in degrees\n",
        "        ]\n",
        "    else:\n",
        "        scaled_sizes = [min_size for _ in g.vs]\n",
        "    g.vs[\"size\"] = scaled_sizes\n",
        "\n",
        "    # ノードサイズに基づき再描画（ノードの大きさ＝次数）\n",
        "    fig, ax = plt.subplots(figsize=figsize)\n",
        "    ig.plot(\n",
        "        g,\n",
        "        target=ax,\n",
        "        layout=layout,\n",
        "        vertex_size=g.vs[\"size\"],\n",
        "        vertex_label=g.vs[\"label\"],\n",
        "        vertex_label_size=vertex_label_size,\n",
        "        vertex_color=g.vs[\"color\"],\n",
        "        edge_width=edge_widths,\n",
        "        edge_color=edge_color,\n",
        "        edge_curved=edge_curved,\n",
        "        bbox=(figsize[0]*100, figsize[1]*100),\n",
        "        margin=40,\n",
        "    )\n",
        "    plt.show()\n",
        "    return g,communities"
      ],
      "metadata": {
        "id": "2Ha67lIWsGp3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ノードのページランク指標に応じてノードサイズを変更\n",
        "def plot_igraph_from_adjacency_pr(\n",
        "    adjacency_matrix,\n",
        "    labels,\n",
        "    layout_type=\"fr\",\n",
        "    vertex_size=30,\n",
        "    edge_width_scale=5,\n",
        "    vertex_label_size=14,\n",
        "    edge_curved=False,\n",
        "    figsize=(8, 8),\n",
        "    edge_threshold=0.0,\n",
        "    vertex_color=\"skyblue\",\n",
        "    edge_color=\"gray\",\n",
        "    top_n_labels=20\n",
        "):\n",
        "    import igraph as ig\n",
        "    import matplotlib.pyplot as plt\n",
        "    import numpy as np\n",
        "\n",
        "    # 隣接行列をnumpy配列に変換\n",
        "    adj = np.array(adjacency_matrix)\n",
        "    n = adj.shape[0]\n",
        "\n",
        "    # エッジの重みが閾値以下のものは0にする\n",
        "    adj = np.where(adj > edge_threshold, adj, 0)\n",
        "\n",
        "    # igraphグラフ作成\n",
        "    g = ig.Graph.Weighted_Adjacency(adj.tolist(), mode=ig.ADJ_UNDIRECTED, attr=\"weight\", loops=False)\n",
        "    g.vs[\"name\"] = labels\n",
        "\n",
        "    # 孤立ノード除去\n",
        "    isolated_nodes = [v.index for v in g.vs if g.degree(v) == 0.0]\n",
        "    if isolated_nodes:\n",
        "        g.delete_vertices(isolated_nodes)\n",
        "\n",
        "\n",
        "\n",
        "    # ノードラベル\n",
        "    # labels = vocab として渡される前提\n",
        "    if \"name\" in g.vs.attributes():\n",
        "        # g.vs[\"name\"] から g.vs[\"label\"] へコピー\n",
        "        g.vs[\"label\"] = g.vs[\"name\"]\n",
        "    else:\n",
        "        # \"name\" がない場合のフォールバック\n",
        "        g.vs[\"label\"] = [str(i) for i in range(g.vcount())]\n",
        "    # 次数の高いノードの上位top_n_labels件のみラベルを付与\n",
        "    degrees = g.pagerank()\n",
        "    top_indices = np.argsort(degrees)[-top_n_labels:]  # 上位top_n_labels件\n",
        "    g.vs[\"label\"] = [\n",
        "        g.vs[i][\"label\"] if i in top_indices else \"\" for i in range(g.vcount())\n",
        "    ]\n",
        "    # レイアウト\n",
        "    layout = g.layout(layout_type)\n",
        "\n",
        "    # エッジの太さ\n",
        "    edge_weights = g.es[\"weight\"] if \"weight\" in g.es.attributes() else [1]*g.ecount()\n",
        "    if len(edge_weights) > 0 and max(edge_weights) > 0:\n",
        "        edge_widths = [edge_width_scale * (w / max(edge_weights)) for w in edge_weights]\n",
        "    else:\n",
        "        edge_widths = [1 for _ in edge_weights]\n",
        "\n",
        "    # コミュニティ検出（例: Louvain法, fallback to fastgreedy if Louvain not available）\n",
        "    try:\n",
        "        communities = g.community_multilevel(weights=g.es['weight'] if 'weight' in g.es.attributes() else None)\n",
        "    except AttributeError:\n",
        "        # fallback method\n",
        "        communities = g.community_fastgreedy(weights=g.es['weight'] if 'weight' in g.es.attributes() else None).as_clustering()\n",
        "    membership = communities.membership\n",
        "    import matplotlib\n",
        "    cmap = matplotlib.cm.get_cmap('tab20')\n",
        "    num_colors = cmap.N if hasattr(cmap, \"N\") else 20\n",
        "    comm_vertex_colors = [matplotlib.colors.to_hex(cmap(i % num_colors)) for i in membership]\n",
        "    g.vs[\"color\"] = comm_vertex_colors\n",
        "    # ノードの次数に応じてノードサイズを変更する\n",
        "    degrees = g.pagerank()\n",
        "    # 適度なスケーリングのために最小サイズと最大サイズを指定\n",
        "    min_size = 20\n",
        "    max_size = 80\n",
        "    # スケーリング\n",
        "    if degrees:\n",
        "        min_deg = min(degrees)\n",
        "        max_deg = max(degrees) if max(degrees) != min(degrees) else min(degrees) + 1\n",
        "        # ノードサイズ算出\n",
        "        scaled_sizes = [\n",
        "            min_size + (deg - min_deg) / (max_deg - min_deg) * (max_size - min_size)\n",
        "            for deg in degrees\n",
        "        ]\n",
        "    else:\n",
        "        scaled_sizes = [min_size for _ in g.vs]\n",
        "    g.vs[\"size\"] = scaled_sizes\n",
        "\n",
        "    # ノードサイズに基づき再描画（ノードの大きさ＝次数）\n",
        "    fig, ax = plt.subplots(figsize=figsize)\n",
        "    ig.plot(\n",
        "        g,\n",
        "        target=ax,\n",
        "        layout=layout,\n",
        "        vertex_size=g.vs[\"size\"],\n",
        "        vertex_label=g.vs[\"label\"],\n",
        "        vertex_label_size=vertex_label_size,\n",
        "        vertex_color=g.vs[\"color\"],\n",
        "        edge_width=edge_widths,\n",
        "        edge_color=edge_color,\n",
        "        edge_curved=edge_curved,\n",
        "        bbox=(figsize[0]*100, figsize[1]*100),\n",
        "        margin=40,\n",
        "    )\n",
        "    plt.show()\n",
        "    return g,communities"
      ],
      "metadata": {
        "id": "N5MhLOxosJ20"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# コミュニティ検出なしでネットワーク構築\n",
        "def plot_network_nocom(\n",
        "    adjacency_matrix,\n",
        "    labels,\n",
        "    layout_type=\"fr\",\n",
        "    vertex_size=30,\n",
        "    edge_width_scale=5,\n",
        "    vertex_label_size=14,\n",
        "    edge_curved=False,\n",
        "    figsize=(8, 8),\n",
        "    edge_threshold=0.0,\n",
        "    vertex_color=\"skyblue\",\n",
        "    edge_color=\"gray\",\n",
        "    top_n_labels=20\n",
        "):\n",
        "    import igraph as ig\n",
        "    import matplotlib.pyplot as plt\n",
        "    import numpy as np\n",
        "\n",
        "    # 隣接行列をnumpy配列に変換\n",
        "    adj = np.array(adjacency_matrix)\n",
        "    n = adj.shape[0]\n",
        "\n",
        "    # エッジの重みが閾値以下のものは0にする\n",
        "    adj = np.where(adj > edge_threshold, adj, 0)\n",
        "\n",
        "    # igraphグラフ作成\n",
        "    g = ig.Graph.Weighted_Adjacency(adj.tolist(), mode=ig.ADJ_UNDIRECTED, attr=\"weight\", loops=False)\n",
        "    g.vs[\"name\"] = labels\n",
        "\n",
        "    # 孤立ノード除去\n",
        "    isolated_nodes = [v.index for v in g.vs if g.degree(v) == 0.0]\n",
        "    if isolated_nodes:\n",
        "        g.delete_vertices(isolated_nodes)\n",
        "\n",
        "\n",
        "\n",
        "    # ノードラベル\n",
        "    # labels = vocab として渡される前提\n",
        "    if \"name\" in g.vs.attributes():\n",
        "        # g.vs[\"name\"] から g.vs[\"label\"] へコピー\n",
        "        g.vs[\"label\"] = g.vs[\"name\"]\n",
        "    else:\n",
        "        # \"name\" がない場合のフォールバック\n",
        "        g.vs[\"label\"] = [str(i) for i in range(g.vcount())]\n",
        "    # 次数の高いノードの上位top_n_labels件のみラベルを付与\n",
        "    degrees = g.degree()\n",
        "    top_indices = np.argsort(degrees)[-top_n_labels:]  # 上位top_n_labels件\n",
        "    g.vs[\"label\"] = [\n",
        "        g.vs[i][\"label\"] if i in top_indices else \"\" for i in range(g.vcount())\n",
        "    ]\n",
        "    # レイアウト\n",
        "    layout = g.layout(layout_type)\n",
        "\n",
        "    # エッジの太さ\n",
        "    edge_weights = g.es[\"weight\"] if \"weight\" in g.es.attributes() else [1]*g.ecount()\n",
        "    if len(edge_weights) > 0 and max(edge_weights) > 0:\n",
        "        edge_widths = [edge_width_scale * (w / max(edge_weights)) for w in edge_weights]\n",
        "    else:\n",
        "        edge_widths = [1 for _ in edge_weights]\n",
        "\n",
        "    # ノードの次数に応じてノードサイズを変更する\n",
        "    degrees = g.degree()\n",
        "    # 適度なスケーリングのために最小サイズと最大サイズを指定\n",
        "    min_size = 20\n",
        "    max_size = 80\n",
        "    # スケーリング\n",
        "    if degrees:\n",
        "        min_deg = min(degrees)\n",
        "        max_deg = max(degrees) if max(degrees) != min(degrees) else min(degrees) + 1\n",
        "        # ノードサイズ算出\n",
        "        scaled_sizes = [\n",
        "            min_size + (deg - min_deg) / (max_deg - min_deg) * (max_size - min_size)\n",
        "            for deg in degrees\n",
        "        ]\n",
        "    else:\n",
        "        scaled_sizes = [min_size for _ in g.vs]\n",
        "    g.vs[\"size\"] = scaled_sizes\n",
        "\n",
        "    # ノードサイズに基づき再描画（ノードの大きさ＝次数）\n",
        "    fig, ax = plt.subplots(figsize=figsize)\n",
        "    ig.plot(\n",
        "        g,\n",
        "        target=ax,\n",
        "        layout=layout,\n",
        "        vertex_size=g.vs[\"size\"],\n",
        "        vertex_label=g.vs[\"label\"],\n",
        "        vertex_label_size=vertex_label_size,\n",
        "        vertex_color=vertex_color,\n",
        "        edge_width=edge_widths,\n",
        "        edge_color=edge_color,\n",
        "        edge_curved=edge_curved,\n",
        "        bbox=(figsize[0]*100, figsize[1]*100),\n",
        "        margin=40,\n",
        "    )\n",
        "    plt.show()\n",
        "    return g"
      ],
      "metadata": {
        "id": "HnoB9c2JsNL5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import igraph as ig\n",
        "from scipy import sparse\n",
        "\n",
        "def get_node_metrics_fast(graph, communities):\n",
        "    \"\"\"\n",
        "    行列演算を用いた高速版:\n",
        "    全ノードに対して参加係数、z-score、Betweenness、PageRankを計算し、\n",
        "    全件のDataFrameを返す（フィルタリングはしない）\n",
        "    \"\"\"\n",
        "    # コミュニティ情報の取得\n",
        "    membership = np.array(communities.membership)\n",
        "    num_communities = len(communities)\n",
        "    num_nodes = graph.vcount()\n",
        "\n",
        "    # --- 高速化の前準備: 疎行列の作成 ---\n",
        "    A = graph.get_adjacency_sparse()\n",
        "\n",
        "    # U: 所属行列\n",
        "    row_indices = np.arange(num_nodes)\n",
        "    col_indices = membership\n",
        "    data = np.ones(num_nodes)\n",
        "    U = sparse.csr_matrix((data, (row_indices, col_indices)), shape=(num_nodes, num_communities))\n",
        "\n",
        "    # K_dist: コミュニティ別次数行列\n",
        "    K_dist = A @ U\n",
        "\n",
        "    # k_i: 各ノードの全次数\n",
        "    k_i = np.array(A.sum(axis=1)).flatten()\n",
        "\n",
        "    # --- A. 参加係数 (Participation Coefficient) の計算 ---\n",
        "    k_i_safe = np.where(k_i == 0, 1, k_i)\n",
        "    K_dist_sq = K_dist.power(2)\n",
        "    sum_k_is_sq = np.array(K_dist_sq.sum(axis=1)).flatten()\n",
        "    participation_coeffs = 1.0 - (sum_k_is_sq / (k_i_safe ** 2))\n",
        "    participation_coeffs[k_i == 0] = 0.0\n",
        "\n",
        "    # --- B. コミュニティ内次数 (z-score) の計算 ---\n",
        "    k_in = np.array(K_dist[np.arange(num_nodes), membership]).flatten()\n",
        "\n",
        "    df_temp = pd.DataFrame({\n",
        "        'comm_id': membership,\n",
        "        'k_in': k_in\n",
        "    })\n",
        "\n",
        "    grouped = df_temp.groupby('comm_id')['k_in']\n",
        "    means = grouped.transform('mean')\n",
        "    stds = grouped.transform('std')\n",
        "    stds = stds.replace(0, 1)\n",
        "\n",
        "    z_scores = (df_temp['k_in'] - means) / stds\n",
        "    z_scores = z_scores.fillna(0).values\n",
        "\n",
        "    # --- C. データのまとめ ---\n",
        "\n",
        "    # PageRank等の計算\n",
        "    degree = k_i\n",
        "    betweenness = graph.betweenness()\n",
        "    pagerank = graph.pagerank()\n",
        "\n",
        "    # 全ノードを含むDataFrameを作成\n",
        "    df = pd.DataFrame({\n",
        "        \"name\": graph.vs[\"name\"],\n",
        "        \"community_id\": membership, # ここにLouvainの結果(0, 1, 2...)が入ります\n",
        "        \"degree\": degree,\n",
        "        \"pagerank\": pagerank,\n",
        "        \"betweenness\": betweenness,\n",
        "        \"z_score\": z_scores,\n",
        "        \"participation\": participation_coeffs\n",
        "    })\n",
        "\n",
        "    # ここではまだフィルタリングせず、全データを返します\n",
        "    return df\n",
        "\n",
        "\n",
        "def get_top10_per_community(graph, communities):\n",
        "    \"\"\"\n",
        "    1. 全ノードの指標を一度に計算する\n",
        "    2. 各コミュニティごとに、重要度（ここではdegree）が高い順に10件を抽出する\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. 全ノードの計算（ループは使いません）\n",
        "    all_nodes_df = get_node_metrics_fast(graph, communities)\n",
        "\n",
        "    # 2. Pandasの機能で「コミュニティごとに上位10件」を抽出\n",
        "    #    ソート順位を変えたい場合は by=\"pagerank\" などに変更してください\n",
        "    final_df = (all_nodes_df\n",
        "                .sort_values([\"community_id\", \"degree\"], ascending=[True, False])\n",
        "                .groupby(\"community_id\")\n",
        "                .head(10)  # 各グループの上位10件を取得\n",
        "                )\n",
        "\n",
        "    return final_df,all_nodes_df"
      ],
      "metadata": {
        "id": "_RhrVcG9sW4Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_network_scatter(df,\n",
        "                         x_col=\"degree\",\n",
        "                         y_col=\"betweenness\",\n",
        "                         hue_col=\"community_id\",\n",
        "                         size_col=\"pagerank\",\n",
        "                         name_col=\"name\",        # 追加: ノード名が入っているカラム名\n",
        "                         label_threshold_x=None,\n",
        "                         figsize=(12, 8)):\n",
        "    \"\"\"\n",
        "    ネットワーク指標の散布図を描画し、nameカラムの値を使ってラベルを付ける関数\n",
        "    \"\"\"\n",
        "\n",
        "    # 図の準備\n",
        "    plt.figure(figsize=figsize)\n",
        "\n",
        "    # community_idなどが数値の場合、カテゴリとして扱うために文字列に変換\n",
        "    plot_df = df.copy()\n",
        "    if hue_col in plot_df.columns:\n",
        "        plot_df[hue_col] = plot_df[hue_col].astype(str)\n",
        "\n",
        "    # 散布図の描画\n",
        "    sns.scatterplot(\n",
        "        data=plot_df,\n",
        "        x=x_col,\n",
        "        y=y_col,\n",
        "        hue=hue_col,\n",
        "        size=size_col,\n",
        "        sizes=(50, 600),\n",
        "        alpha=0.7,\n",
        "        palette=\"tab10\",\n",
        "        edgecolor=\"black\"\n",
        "    )\n",
        "\n",
        "    # --- ラベルの付与処理 (修正箇所) ---\n",
        "    if label_threshold_x is not None:\n",
        "        # 閾値条件を満たす行だけ抽出\n",
        "        labels_df = df[df[y_col] >= label_threshold_x]\n",
        "\n",
        "        # 行ごとにループ処理 (indexではなく、rowから情報を取得)\n",
        "        for _, row in labels_df.iterrows():\n",
        "            plt.text(\n",
        "                x=row[x_col],\n",
        "                y=row[y_col],\n",
        "                s=row[name_col],  # 修正: 指定したカラム(name)の値を使用\n",
        "                fontsize=13,\n",
        "                color='black',\n",
        "                fontweight='bold',\n",
        "                ha='right',\n",
        "                va='bottom'\n",
        "            )\n",
        "\n",
        "    # グリッドやタイトルの設定\n",
        "    plt.title(f\"Network Metrics: {x_col} vs {y_col}\", fontsize=20)\n",
        "    plt.xlabel(x_col, fontsize=20)\n",
        "    plt.ylabel(y_col, fontsize=20)\n",
        "    plt.grid(True, linestyle='--', alpha=0.6)\n",
        "\n",
        "    # 凡例を枠外に出す\n",
        "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0,fontsize=20)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "ePoepbiBsZBX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 実行"
      ],
      "metadata": {
        "id": "jXJav-x8tlXA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tt_df = pd.read_csv(\"h2o/matrix/tt_loyal.csv\")\n",
        "tt_= tt_df.to_numpy()"
      ],
      "metadata": {
        "id": "Vv98KEy1u08e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "adj_matrix = adj_matrix_(tt_,vocab,threshold=0)\n",
        "find_threshold_by_count(adj_matrix, target_edge_count=900)"
      ],
      "metadata": {
        "id": "aC0pvXkstBi7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "g,communities = plot_igraph_from_adjacency_pr(adj_matrix,labels=vocab,edge_threshold=0.0006503,top_n_labels=15,vertex_label_size=8,vertex_size=50)\n",
        "community_top10,all_nodes_df = get_top10_per_community(g, communities)"
      ],
      "metadata": {
        "id": "nbp1PNWos1Iq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_network_scatter(all_nodes_df, x_col=\"degree\", y_col=\"betweenness\", hue_col=\"community_id\", size_col=\"pagerank\",name_col=\"name\", label_threshold_x=2000, figsize=(12, 8))"
      ],
      "metadata": {
        "id": "b354i_dMs37L"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}