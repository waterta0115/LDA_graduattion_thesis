{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM/cfYO4L5IuSgBgkyQ9FBE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/waterta0115/LDA_graduattion_thesis/blob/main/LDA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 準備"
      ],
      "metadata": {
        "id": "f6J-BttWCKNc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hIdZVm-SanEU"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "import japanize_matplotlib\n",
        "import arviz as az\n",
        "import matplotlib.dates as mdates\n",
        "from itertools import combinations\n",
        "import igraph as ig\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "az.style.use(\"arviz-doc\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import, division, unicode_literals  # noqa\n",
        "import logging\n",
        "import sys\n",
        "import lda._lda\n",
        "import lda.utils\n",
        "from scipy.special import digamma, polygamma\n",
        "\n",
        "# ログ出力の設定(1)\n",
        "logger = logging.getLogger('lda')\n",
        "\n",
        "class LDA:\n",
        "    def __init__(self, n_topics, n_iter=2000, update_alpha=False,update_eta=False,alpha=0.1, eta=0.01, random_state=None,\n",
        "                 refresh=10):\n",
        "        self.n_topics = n_topics\n",
        "        self.n_iter = n_iter\n",
        "        self.update_alpha = update_alpha\n",
        "        self.alpha = alpha\n",
        "        self.update_eta = update_eta\n",
        "        self.eta = eta\n",
        "\n",
        "        # if random_state is None, check_random_state(None) does nothing\n",
        "        # other than return the current numpy RandomState\n",
        "        self.random_state = random_state\n",
        "        self.refresh = refresh\n",
        "\n",
        "        alpha_is_valid = alpha > 0 if isinstance(alpha, (int, float)) else np.all(alpha > 0)\n",
        "        if not alpha_is_valid or eta <= 0:\n",
        "             raise ValueError(\"alpha and eta must be greater than zero\")\n",
        "\n",
        "        # random numbers that are reused\n",
        "        rng = lda.utils.check_random_state(random_state)\n",
        "        self._rands = rng.rand(1024**2 // 8)  # 1MiB of random variates\n",
        "\n",
        "        # configure console logging if not already configured\n",
        "        # ログ出力の設定(2)\n",
        "        if len(logger.handlers) == 1 and isinstance(logger.handlers[0], logging.NullHandler):\n",
        "            logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        self._fit(X)\n",
        "        return self\n",
        "\n",
        "    def fit_transform(self, X, y=None):\n",
        "        if isinstance(X, np.ndarray):\n",
        "            # in case user passes a (non-sparse) array of shape (n_features,)\n",
        "            # turn it into an array of shape (1, n_features)\n",
        "            X = np.atleast_2d(X)\n",
        "        self._fit(X)\n",
        "        return self.doc_topic_\n",
        "\n",
        "    def transform(self, X, max_iter=20, tol=1e-16):\n",
        "        if isinstance(X, np.ndarray):\n",
        "            # in case user passes a (non-sparse) array of shape (n_features,)\n",
        "            # turn it into an array of shape (1, n_features)\n",
        "            X = np.atleast_2d(X)\n",
        "        doc_topic = np.empty((X.shape[0], self.n_topics))\n",
        "            # 結果格納用の配列を用意\n",
        "        WS, DS = lda.utils.matrix_to_lists(X)\n",
        "            # 行列→リスト　WS：単語インデックス　DS：その単語がどの文書に属しているか　に変換\n",
        "        \"\"\"\n",
        "        Doc1: 単語1が2回, 単語3が1回\n",
        "        Doc2: 単語2が3回\n",
        "        \"\"\"\n",
        "        # この場合\n",
        "        # WS = [1, 1, 3, 2, 2, 2]\n",
        "        # DS = [0, 0, 0, 1, 1, 1]\n",
        "        # このようにWSとDSは同じ長さとなる\n",
        "\n",
        "        # TODO: this loop is parallelizable\n",
        "        for d in np.unique(DS):\n",
        "            doc_topic[d] = self._transform_single(WS[DS == d], max_iter, tol)\n",
        "        return doc_topic\n",
        "\n",
        "    def _transform_single(self, doc, max_iter, tol):\n",
        "        # 内部的に doc = WS[DS == d] のような変換が行われている(あるdに属している単語集合)\n",
        "        # ここでdocは「文書中の単語たち」を表す 単語インデックスの配列\n",
        "        PZS = np.zeros((len(doc), self.n_topics))\n",
        "        for iteration in range(max_iter + 1):  # +1 is for initialization\n",
        "            PZS_new = self.components_[:, doc].T\n",
        "                # components_はトピック-単語分布行列(phi)\n",
        "                # その中からdoc内に含まれている単語だけを抽出して転置\n",
        "                # PZS_newは各単語が「どのトピックにどれくらい関係しているか」の確率表\n",
        "            PZS_new *= (PZS.sum(axis=0) - PZS + self.alpha)\n",
        "                # PZS.sum(axis=0)：各文書はどれくらいの数のトピックを持っているか\n",
        "                    # ここではブロードキャスト機能が自動的に働くため、PZSの次元は(1,n_topics)となる\n",
        "                # PZS.sum(axis=0)-PZS：shape==(n_words_in_doc,n_topics)\n",
        "                # self.alphaはスカラー又は(n_topics,)であるが、ブロードキャストにより全体の次元は(n_words_in_doc,n_topics)\n",
        "            PZS_new /= PZS_new.sum(axis=1)[:, np.newaxis]  # vector to single column matrix\n",
        "                # np.array([a,b,c])[:,np.newaxis]とすることで1次元配列を2次元配列にすることが可能\n",
        "                    # (3,)→(3,1)；これらは全くの別物\n",
        "                # ここでは確率にしている（正規化）\n",
        "            delta_naive = np.abs(PZS_new - PZS).sum()\n",
        "                # 前回の値との変化量\n",
        "            logger.debug('transform iter {}, delta {}'.format(iteration, delta_naive))\n",
        "            PZS = PZS_new\n",
        "            if delta_naive < tol:\n",
        "                break\n",
        "                # 収束判定\n",
        "        theta_doc = PZS.sum(axis=0) / PZS.sum()\n",
        "            # 文書内のすべての単語にわたるトピック確率を合計し、全体で正規化して文書レベルのトピック分布を作る\n",
        "        assert len(theta_doc) == self.n_topics\n",
        "        assert theta_doc.shape == (self.n_topics,)\n",
        "        return theta_doc\n",
        "\n",
        "    def _fit(self, X):\n",
        "        \"\"\"inference by \"\"\"\n",
        "        random_state = lda.utils.check_random_state(self.random_state)\n",
        "        rands = self._rands.copy()\n",
        "            # randsは事前に生成された乱数列のコピー\n",
        "            # gibbs samplingで使う乱数をここから供給する\n",
        "        self._initialize(X)\n",
        "            # 各単語にランダムにトピックを割り当てる\n",
        "            # 各カウント行列を作成：nzw_,ndz_,nz_\n",
        "                # nzw_：topic-wordの共起数\n",
        "                # ndz_：doc-topicの共起数\n",
        "                # nz_：各トピックに属する単語数の合計\n",
        "        for it in range(self.n_iter):\n",
        "            # FIXME: using numpy.roll with a random shift might be faster\n",
        "            random_state.shuffle(rands)\n",
        "            if it % self.refresh == 0:\n",
        "                ll = self.loglikelihood()\n",
        "                logger.info(\"<{}> log likelihood: {:.0f}\".format(it, ll))\n",
        "                # keep track of loglikelihoods for monitoring convergence\n",
        "                self.loglikelihoods_.append(ll)\n",
        "            self._sample_topics(rands)\n",
        "            if it == 1:\n",
        "                print(\"ndz 初回サンプル:\",self.ndz_[:5])\n",
        "            if it == 100:\n",
        "                print(\"ndz 100回目:\", self.ndz_[:5])\n",
        "            if self.update_alpha:     # ユーザが制御できるようにオプション化\n",
        "                self._update_alpha()\n",
        "                if it % 10 == 0:\n",
        "                    logger.info(f\"iteration{it}: alpha = {self.alpha}\")\n",
        "            if self.update_eta:     # ユーザが制御できるようにオプション化\n",
        "                self._update_eta()\n",
        "                if it % 10 == 0:\n",
        "                    logger.info(f\"iteration{it}: alpha = {self.eta}\")\n",
        "            # ここがcollapsed gibbs samplingの実体。lda-projectの内部で構築されている\n",
        "        ll = self.loglikelihood()\n",
        "            # 最終的な対数尤度の計算・出力\n",
        "        logger.info(\"<{}> log likelihood: {:.0f}\".format(self.n_iter - 1, ll))\n",
        "        # note: numpy /= is integer division\n",
        "        self.components_ = (self.nzw_ + self.eta).astype(float)\n",
        "            # components_とは各トピックごとの単語分布（phi）：(n_topics,n_words)\n",
        "            # etaを加えることでスムージングしている\n",
        "        self.components_ /= np.sum(self.components_, axis=1)[:, np.newaxis]\n",
        "            # 確率にしている（正規化）\n",
        "        self.topic_word_ = self.components_\n",
        "            # scikit-learn互換用のために同じ内容を別名で保存\n",
        "        self.doc_topic_ = (self.ndz_ + self.alpha).astype(float)\n",
        "            # doc_topic_とは各文書ごとのトピック分布(theta)：(n_docs,n_topics)\n",
        "        self.doc_topic_ /= np.sum(self.doc_topic_, axis=1)[:, np.newaxis]\n",
        "\n",
        "        # delete attributes no longer needed after fitting to save memory and reduce clutter\n",
        "        del self.WS\n",
        "        del self.DS\n",
        "        del self.ZS\n",
        "        return self\n",
        "            # components_とdoc_topic_とloglikelihoods_を返す\n",
        "\n",
        "    def _initialize(self, X):\n",
        "        D, W = X.shape\n",
        "        N = int(X.sum())\n",
        "        n_topics = self.n_topics\n",
        "        n_iter = self.n_iter\n",
        "        logger.info(\"n_documents: {}\".format(D))\n",
        "        logger.info(\"vocab_size: {}\".format(W))\n",
        "        logger.info(\"n_words: {}\".format(N))\n",
        "        logger.info(\"n_topics: {}\".format(n_topics))\n",
        "        logger.info(\"n_iter: {}\".format(n_iter))\n",
        "\n",
        "        self.nzw_ = nzw_ = np.zeros((n_topics, W), dtype=np.intc, order=\"F\")\n",
        "        self.ndz_ = ndz_ = np.zeros((D, n_topics), dtype=np.intc, order=\"C\")\n",
        "        self.nz_ = nz_ = np.zeros(n_topics, dtype=np.intc)\n",
        "\n",
        "        self.WS, self.DS = WS, DS = lda.utils.matrix_to_lists(X)\n",
        "        self.ZS = ZS = np.empty_like(self.WS, dtype=np.intc)\n",
        "        np.testing.assert_equal(N, len(WS))\n",
        "        for i in range(N):\n",
        "            w,d = WS[i], DS[i]\n",
        "            z_new = i % n_topics\n",
        "            ZS[i] = z_new\n",
        "            ndz_[d, z_new] += 1\n",
        "            nzw_[z_new, w] += 1\n",
        "            nz_[z_new] += 1\n",
        "        self.loglikelihoods_ = []\n",
        "\n",
        "    def loglikelihood(self):# ここではalphaやetaはスカラーであることが想定されている\n",
        "        nzw, ndz, nz = self.nzw_, self.ndz_, self.nz_\n",
        "        alpha = self.alpha\n",
        "        eta = self.eta\n",
        "        nd = np.sum(ndz, axis=1).astype(np.intc)\n",
        "        if isinstance(alpha, np.ndarray) and alpha.ndim > 0:\n",
        "            alpha_scalar = alpha[0]\n",
        "        else:\n",
        "            alpha_scalar = alpha\n",
        "        if isinstance(eta, np.ndarray) and eta.ndim > 0:\n",
        "            eta_scalar = eta[0]\n",
        "        else:\n",
        "            eta_scalar = eta\n",
        "        return lda._lda._loglikelihood(nzw, ndz, nz, nd, alpha_scalar, eta_scalar)\n",
        "\n",
        "    def _sample_topics(self, rands):# ここではalphaやetaはK次元であることが想定されている\n",
        "        n_topics, vocab_size = self.nzw_.shape\n",
        "        if isinstance(self.alpha, (int, float)):\n",
        "            alpha_array = np.full(n_topics, self.alpha)\n",
        "        else:\n",
        "            alpha_array = self.alpha\n",
        "        if isinstance(self.eta, (int, float)):\n",
        "            eta_array = np.full(n_topics, self.eta)\n",
        "        else:\n",
        "            eta_array = self.eta\n",
        "        alpha = alpha_array.astype(np.float64)\n",
        "        eta = eta_array.astype(np.float64)\n",
        "        lda._lda._sample_topics(self.WS, self.DS, self.ZS, self.nzw_, self.ndz_, self.nz_,\n",
        "                                alpha, eta, rands)\n",
        "\n",
        "    import numpy as np\n",
        "    from scipy.special import digamma, polygamma\n",
        "    def _update_alpha(self):\n",
        "        if not hasattr(self, \"ndz_\"):\n",
        "            raise ValueError(\"ndz_ が存在しません\")\n",
        "\n",
        "        if np.all(self.ndz_ == self.ndz_[0]):\n",
        "            print(\"警告: ndz_ が全ドキュメント同じ値になっています\")\n",
        "\n",
        "        dt = self.ndz_              # shape (D, K)\n",
        "        D, K = dt.shape\n",
        "\n",
        "        # ensure alpha is scalar\n",
        "        if isinstance(self.alpha, np.ndarray):\n",
        "            alpha_scalar = float(self.alpha[0])\n",
        "        else:\n",
        "            alpha_scalar = float(self.alpha) # float型の場合はそのままfloatに変換\n",
        "\n",
        "        # s = digamma(dt + alpha_scalar).sum(axis=0)\n",
        "        # g = s - D * digamma(alpha_scalar)\n",
        "        # h = -D * polygamma(1, alpha_scalar)\n",
        "        # alpha_new = alpha_scalar - g / h\n",
        "        for i in range(100):\n",
        "            N_d = np.sum(dt,axis=1)\n",
        "            term1_num = np.sum(digamma(dt+alpha_scalar))\n",
        "            term2_num = D*K*digamma(alpha_scalar)\n",
        "            numerator = term1_num - term2_num\n",
        "            term1_den = K*np.sum(digamma(N_d+alpha_scalar*K))\n",
        "            term2_den = D*K*digamma(alpha_scalar*K)\n",
        "            denominator = term1_den - term2_den\n",
        "            alpha_new = alpha_scalar*(numerator/denominator)\n",
        "            diff = np.abs(alpha_new-alpha_scalar)\n",
        "            print(f\"Iter{i+1}:alpha={alpha_new:.6f}(diff={diff:.6e})\")\n",
        "            if diff<1e-05:\n",
        "                print(\"収束しました\")\n",
        "                break\n",
        "            alpha_scalar = alpha_new\n",
        "        # store symmetric vector\n",
        "        self.alpha = np.ones(K)*alpha_new\n",
        "\n",
        "        # logging\n",
        "        logger.info(f\"alpha updated (symmetric): {alpha_new}\")\n",
        "\n",
        "        if not hasattr(self, \"alpha_history\"):\n",
        "            self.alpha_history = []\n",
        "        self.alpha_history.append(self.alpha.copy())\n",
        "    def _update_eta(self):\n",
        "        if not hasattr(self, \"nzw_\"):\n",
        "            raise ValueError(\"nzw_ が存在しません\")\n",
        "\n",
        "        nzw = self.nzw_              # shape (K, V)\n",
        "        nz = self.nz_               # shape (K,)\n",
        "        K, V = nzw.shape\n",
        "\n",
        "        # ensure eta is scalar\n",
        "        if isinstance(self.eta, np.ndarray):\n",
        "            eta_scalar = float(self.eta[0])\n",
        "        else:\n",
        "            eta_scalar = float(self.eta)\n",
        "\n",
        "        # fixed-point iteration (symmetric eta)\n",
        "        for i in range(100):\n",
        "            term1_num = np.sum(digamma(nzw + eta_scalar))     # Σ_k Σ_w ψ(n_kw + η)\n",
        "            term2_num = K * V * digamma(eta_scalar)           # K*V ψ(η)\n",
        "            numerator = term1_num - term2_num\n",
        "\n",
        "            term1_den = V * np.sum(digamma(nz + V * eta_scalar))  # V Σ_k ψ(n_k· + Vη)\n",
        "            term2_den = K * V * digamma(V * eta_scalar)           # K*V ψ(Vη)\n",
        "            denominator = term1_den - term2_den\n",
        "\n",
        "            eta_new = eta_scalar * (numerator / denominator)\n",
        "            diff = abs(eta_new - eta_scalar)\n",
        "\n",
        "            print(f\"[eta] Iter{i+1}: eta={eta_new:.6f} (diff={diff:.6e})\")\n",
        "\n",
        "            if diff < 1e-5:\n",
        "                print(\"eta 収束しました\")\n",
        "                break\n",
        "\n",
        "            eta_scalar = eta_new\n",
        "\n",
        "        # store symmetric eta vector\n",
        "        self.eta = eta_new\n",
        "\n",
        "        # logging\n",
        "        logger.info(f\"eta updated (symmetric): {eta_new}\")\n",
        "\n",
        "        # store history\n",
        "        if not hasattr(self, \"eta_history\"):\n",
        "            self.eta_history = []\n",
        "        self.eta_history.append(eta_new)"
      ],
      "metadata": {
        "id": "Nm4IUrKJauzR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare(df):\n",
        "    from gensim import corpora\n",
        "    customer_texts = df.groupby(\"顧客ID\")[\"商品カテゴリ\"].apply(list).tolist()\n",
        "    dictionary = corpora.Dictionary(customer_texts)\n",
        "    corpus = [dictionary.doc2bow(text) for text in customer_texts]\n",
        "    return customer_texts,dictionary,corpus"
      ],
      "metadata": {
        "id": "BbthjmxDbBAg"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_bow(posdata):\n",
        "    from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# 1. 仮統合顧客番号ごとに、品目名を空白区切りで連結した「文書」として扱う\n",
        "    customer_docs = (\n",
        "        posdata\n",
        "        .groupby(\"顧客ID\")[\"商品カテゴリ\"]\n",
        "        .apply(lambda items: ' '.join(map(str, items)))\n",
        "    )\n",
        "\n",
        "# 2. CountVectorizerでBOW作成\n",
        "    vectorizer = CountVectorizer(token_pattern='[^ ]+')\n",
        "    bow_matrix = vectorizer.fit_transform(customer_docs)\n",
        "    vocab = vectorizer.get_feature_names_out()\n",
        "# n_samples, n_features\n",
        "    print(\"BOW shape:\", bow_matrix.shape)\n",
        "    return bow_matrix, vocab"
      ],
      "metadata": {
        "id": "80AEu4ZvbIq-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 実行"
      ],
      "metadata": {
        "id": "MgledflSCQGG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "texts,dictionary,corpus = make_bow(posdata)"
      ],
      "metadata": {
        "id": "a9A_16-RxGyt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "from gensim.models.coherencemodel import CoherenceModel\n",
        "\n",
        "start = 2\n",
        "limit = 50\n",
        "step = 2\n",
        "perplexity_vals = []\n",
        "coherence_vals = []\n",
        "\n",
        "BOW,vocab = make_bow(posdata)\n",
        "\n",
        "for n_topic in tqdm(range(start, limit, step), desc=\"Calculating perplexity\"):\n",
        "    lda = LDA(n_topic,alpha=0.1,eta=0.01,update_alpha=True,update_eta=True, random_state = 0, n_iter = 1500)\n",
        "    lda.fit(BOW)\n",
        "\n",
        "    loglik = lda.loglikelihood()    # ← ここが重要！\n",
        "    total_words = np.sum(BOW)\n",
        "    perplexity = np.exp(-loglik / total_words)\n",
        "    perplexity_vals.append(perplexity)\n",
        "    # --- Coherence (gensim の CoherenceModel を利用) ---\n",
        "    topic_word = lda.topic_word_   # shape (n_topics, vocab)\n",
        "    topics = []\n",
        "    n_top_words = 5\n",
        "    for t in range(n_topic):\n",
        "        # topn_uni = topic_word_uni[t].argsort()[:-n_top_words - 1:-1]\n",
        "        # topics_uni.append([vocab_uni[w_id] for w_id in topn_uni])\n",
        "        word_ids = topic_word[t].argsort()[-n_top_words:][::-1]\n",
        "        word_ids = [int(i) for i in word_ids]  # numpy型→python int\n",
        "        words = [dictionary[i] for i in word_ids]\n",
        "        topics.append(words)\n",
        "    cm = CoherenceModel(topics=topics, texts=texts, dictionary=dictionary, coherence='c_npmi')\n",
        "    coherence_vals.append(cm.get_coherence())\n",
        "\n",
        "x = range(start, limit, step)\n",
        "fig, ax1 = plt.subplots(figsize=(12,5))\n",
        "c1 = 'darkturquoise'\n",
        "ax1.plot(x, coherence_vals, 'o-', color=c1)\n",
        "ax1.set_xlabel('Num Topics')\n",
        "ax1.set_ylabel('Coherence', color=c1); ax1.tick_params('y', colors=c1)\n",
        "c2 = 'slategray'\n",
        "ax2 = ax1.twinx()\n",
        "ax2.plot(x, perplexity_vals, 'o-', color=c2)\n",
        "ax2.set_ylabel('Perplexity', color=c2); ax2.tick_params('y', colors=c2)\n",
        "\n",
        "ax1.set_xticks(x)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2R7lskkIbPSV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lda = LDA(n_topics = 6 , random_state = 0, n_iter = 1500)\n",
        "lda.fit(BOW)\n",
        "topic_word = lda.components_  # model.components_ also works\n",
        "n_top_words = 6\n",
        "plt.plot(lda.loglikelihoods_[5:])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5dqbDe17bSm7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pyLDAvis\n",
        "\n",
        "topic_term_dists = lda.topic_word_   # shape: (n_topics, n_terms)\n",
        "doc_topic_dists = lda.doc_topic_     # shape: (n_docs, n_topics)\n",
        "term_frequency = np.asarray(BOW.sum(axis=0)).flatten()  # 各単語の出現回数\n",
        "\n",
        "# --- pyLDAvis形式に変換して可視化 ---\n",
        "vis_data = pyLDAvis.prepare(\n",
        "    topic_term_dists=topic_term_dists,\n",
        "    doc_topic_dists=doc_topic_dists,\n",
        "    doc_lengths=BOW.sum(axis=1).A1,\n",
        "    vocab=vocab,\n",
        "    term_frequency=term_frequency\n",
        ")\n",
        "pyLDAvis.display(vis_data)"
      ],
      "metadata": {
        "id": "vlfwWBQ_bWHO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}